{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # Import necessary libraries\
import pandas as pd\
import numpy as np\
import matplotlib.pyplot as plt\
import seaborn as sns\
\
# Enable inline plotting for visualization\
%matplotlib inline\
\
# Suppress warnings\
import warnings\
warnings.filterwarnings('ignore')\
\
# For splitting the dataset and evaluating the model\
from sklearn.model_selection import train_test_split\
from xgboost import XGBClassifier  # Import XGBoost classifier\
from sklearn.metrics import accuracy_score, precision_score, recall_score\
\
# Upload and load your dataset\
from google.colab import files\
uploaded = files.upload()\
\
# Load the dataset into a DataFrame (replace 'your_file.csv' with the actual file name after upload)\
df = pd.read_csv(list(uploaded.keys())[0])\
\
# Display the first 5 rows of the dataset\
print("Dataset preview:")\
print(df.head())\
\
# Separate features (X) and target variable (y)\
# Assuming 'target' is the target column, change it if it's different in your dataset\
X = df.drop('target', axis=1)\
y = df['target']\
\
# Split the data into training and testing sets (70% train, 30% test)\
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\
\
# Handle missing values using SimpleImputer (replace missing values with the mean)\
from sklearn.impute import SimpleImputer\
imputer = SimpleImputer(strategy='mean')\
X_train = imputer.fit_transform(X_train)\
X_test = imputer.transform(X_test)\
\
# Initialize the XGBoost Classifier\
xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=0)  # You can adjust these hyperparameters as needed\
\
# Train the XGBoost classifier\
xgb_classifier.fit(X_train, y_train)\
\
# Make predictions on the test set\
y_pred = xgb_classifier.predict(X_test)\
print("\\nPredicted values:", y_pred)\
\
# Calculate precision, recall, and F1-score\
precision = precision_score(y_test, y_pred, average='weighted')\
recall = recall_score(y_test, y_pred, average='weighted')\
f_score = 2 * (precision * recall) / (precision + recall)\
print(f"\\nPrecision: \{precision:.2f\}")\
print(f"Recall: \{recall:.2f\}")\
print(f"F1 Score: \{f_score:.2f\}")\
\
# Calculate accuracy\
accuracy = accuracy_score(y_test, y_pred)\
print(f"Accuracy: \{accuracy:.2f\}")}